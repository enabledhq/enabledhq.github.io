---
layout: post
date: '2017-04-04 14:09 +0930'
published: false
title: 2017-03-04-dynamodb-etl
---
##Designing a DynamoDB ETL with AWS Lambda + s3 Events + KMS

One of our latest projects at Enabled has seen us with a Serverless Architecture, using DynamoDB for our database. A challenge I was recently asked to solve was to build a process for bulk uploading about 13,000 rows of product data in tsv at a time, with a maxium of about 5 of these bulk uploads in our inital production workload.

An example of such a row is:  
```
WHA210-RG	6130791037	CLIWHA210RG	CLI	EA	1	1.00	SWITCH 10A 250V 2P S/MTG IP66 CR/GRY
```
which comes to a total of 77 bytes. The total for the entire file comes to `1.1M`, a long way from big data territory.



Just about every link I click on for performing ETL's into DynamoDB leads me to the same place: AWS Data Pipeline. Data Pipeline is a tool for moving large amounts of data around, and is backed by services such as EMR, Pig and Hive.


- The challenge
- Amazon's Solution
- My Solution

- How it works & Code snippets
- Comparison with other esp. Data Pipeline
	- benchmark results
	- Pricing
    - Ease of use and configuration
    - Lambda timeouts
    	- how to get around this?
- Drawbacks and hurdles
	- S3 triggers! WTF???
    - Serverless and Cloudformation, not so friendly
    
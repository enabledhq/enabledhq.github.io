---
layout: post
date: '2017-04-04 14:09 +0930'
published: false
title: 2017-03-04-dynamodb-etl
---
#Designing a DynamoDB ETL with AWS Lambda + s3 Events + KMS  

  
    
##The Challenge
One of our latest projects at Enabled has seen us with a Serverless Architecture, using DynamoDB for our database. A challenge I was recently asked to solve was to build a process for bulk uploading about 13,000 rows of product data in tsv at a time, with a maxium of about 5 of these bulk uploads in our inital production workload.

An example of such a row is:  
```
WDF0-RR	1153453	CLIWHA210RG	CLI	EA	1	1.40	SWITCH 5A 120V S/MTG IP66 GV/GRY
```
which comes to a total of 77 bytes. The total for the entire file comes to `1.1M`, a long way from big data territory.


##Constraints
We have a number of constraints that are important to take into consideration:

**Changing Inputs**  
Our input data from one distributor to another will have some changes, which could range from different columns, to the need to collapse and perform calculations on the base TSV file. Our system needs to be able to take these differences into account and still bulk upload our data in the same way.

**Encryption**  
Another important constraint for this ETL is that we need to encrypt data at rest. We are already using Amazon's KMS to manage our encryption keys, and the DynamoDBMapper classes in our serverless functions are already set up to integrate with KMS.

##Finding the right porridge

Just about every link I click on for performing ETL's into DynamoDB leads me to the same place: **AWS Data Pipelines**. Data Pipeline is a tool for moving large amounts data from one service to another, or even to or from AWS. It is a utility for stringing a bunch of other services together, such as s3, dynamoDB streams, and EMR clusters.

There are many tutorials for using Data Pipelines to read and write large amounts of data to and from DynamoDB, but the more I read, the more I found that DataPipelines isn't the tool for this job, for the following reasons:
  1. Data pipelines seems a little bit overkill for what we need. I don't want to have to run an EMR cluster (and pay for an EMR cluster) to upload 1.1MB of data
  2. There doesn't seem to be any easy support of DynamoDBMapper and KMS, meaning we would have to write that ourselves. That isn't really a problem, except that we've already written 95% of the code we need to read and write from DynamoDB while encrypting with KMS, it's in our serverless application already!
  
After doing a little more research, I decided that we should implement our own ETL job, using a combination of AWS Lambda, S3, S3 event triggers, and Serverless.

##Proof Of Concept

I started by writing a command line tool in java for parsing JSON files to Java Objects, encrypting and saving to DynamoDB. I wrote this as a part of our existing serverless 'server' (for lack of a better word), in order to reuse the codebase we have already been working on. This also means that maintaining this tool will be much easier later down the track if (read: when) our database changes. 

Here's an example:

```java
import com.fasterxml.jackson.databind.ObjectMapper;

...
ObjectMapper objectMapper = new ObjectMapper();
String jsonString = readFile(pathToJsonFile, Charset.defaultCharset());
seedDataset = objectMapper.readValue(jsonString, SeedDataset.class);
SeedDataset.save(seedDataset, mapper)
...

```
where `SeedDataset` is:
```java
public class SeedDataset {
    private List<DynamoProductMapping>  productMappings = new ArrayList<>();

    @ApiModelProperty(value = "A list of DynamoProductMappings")
    public List<DynamoProductMapping> getProductMappings() {
        return productMappings;
    }

    public void setProductMappings(List<DynamoProductMapping> productMappings) {
        this.productMappings = productMappings;
    }

    public static List<FailedBatch> save(SeedDataset dataset, DynamoDBMapper mapper) {
        List<FailedBatch> failedBatchList = new ArrayList<>();

        failedBatchList.addAll(mapper.batchSave(dataset.getProductMappings()));

        return failedBatchList;
    }
}![Screen Shot 2017-04-04 at 3.34.54 PM.png]({{site.baseurl}}/images/Screen Shot 2017-04-04 at 3.34.54 PM.png)

```

and `DynamoProductMapping` is:
```java
import com.amazonaws.services.dynamodbv2.datamodeling.*;

@DynamoDBTable(tableName = "ProductMapping")
public class DynamoProductMapping {

    private String DistributorId;
    private String iCatItemNo;
    private String unitsOfMeasurement;
    private String distributorSKU;

    public DynamoProductMapping() { }

    @DynamoDBHashKey(attributeName = "DistributorId")
    public String getDistributorId() {
        return DistributorId;
    }

    public void setDistributorId(String distributorId) {
        DistributorId = distributorId;
    }

   ...other getters and setters...
}

```

Finally, our json data  is along the following lines:
```json
{
  "productMappings": [
      {"distributorId":"test:1", "iCatItemNo":"3015-VW",     "distributorSKU":"00001", "unitsOfMeasurement":"[pce]"},
  ...
    ]
 }

```

We ran this test firstly on a local instance of DynamoDB, and then on a live DynamoDB table, testing different settings of the write capacity units, and total number of rows. We managed to confirm that with a write capacity setting of 150 units, we could comfortably write 13,000 rows in about **3 minutes**. Why is this important? Well you will just have to read on!


##Finding the 'Just Right' Solution

Our design:






- How it works & Code snippets
- Comparison with other esp. Data Pipeline
	- benchmark results
	- Pricing
    - Ease of use and configuration
    - Lambda timeouts
    	- how to get around this?
- Drawbacks and hurdles
	- S3 triggers! WTF???
    - Serverless and Cloudformation, not so friendly
    